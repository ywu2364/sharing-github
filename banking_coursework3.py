# -*- coding: utf-8 -*-
"""Banking Coursework3

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ihAnCrRSCkowJZs8cYj8hxhMhw0vqTT9

Preprocessing
"""

# Commented out IPython magic to ensure Python compatibility.
import codecs
import pandas as pd
import numpy as np
import string
import tensorflow as tf
import tensorflow.keras as keras
from keras.preprocessing.text import Tokenizer
from google.colab import drive

# %matplotlib inline

# Commented out IPython magic to ensure Python compatibility.
drive.mount('/gdrive')
# %cd /gdrive

"""load data"""

mydata = pd.read_csv('/gdrive/My Drive/Full_Data.csv',header=0)
mydata.head()

mydata.dtypes

mydata.iloc[:,0].head()

"""Text cleaning for fastText"""

# text cleaning

# eliminate double space
mydata.iloc[:,0] = [" ".join(j.split()) for j in mydata.iloc[:,0]]
#mydata.iloc[:,0] = [j.replace('\'', ' ') for j in mydata.iloc[:,0]]

def load_dict_contractions():
    
    return {
        "ain't":"is not",
        "amn't":"am not",
        "aren't":"are not",
        "can't":"cannot",
        "'cause":"because",
        "couldn't":"could not",
        "couldn't've":"could not have",
        "could've":"could have",
        "daren't":"dare not",
        "daresn't":"dare not",
        "dasn't":"dare not",
        "didn't":"did not",
        "doesn't":"does not",
        "don't":"do not",
        "e'er":"ever",
        "em":"them",
        "everyone's":"everyone is",
        "finna":"fixing to",
        "gimme":"give me",
        "gonna":"going to",
        "gon't":"go not",
        "gotta":"got to",
        "hadn't":"had not",
        "hasn't":"has not",
        "haven't":"have not",
        "he'd":"he would",
        "he'll":"he will",
        "he's":"he is",
        "he've":"he have",
        "how'd":"how would",
        "how'll":"how will",
        "how're":"how are",
        "how's":"how is",
        "I'd":"I would",
        "I'll":"I will",
        "I'm":"I am",
        "I'm'a":"I am about to",
        "I'm'o":"I am going to",
        "isn't":"is not",
        "it'd":"it would",
        "it'll":"it will",
        "it's":"it is",
        "I've":"I have",
        "kinda":"kind of",
        "let's":"let us",
        "mayn't":"may not",
        "may've":"may have",
        "mightn't":"might not",
        "might've":"might have",
        "mustn't":"must not",
        "mustn't've":"must not have",
        "must've":"must have",
        "needn't":"need not",
        "ne'er":"never",
        "o'":"of",
        "o'er":"over",
        "ol'":"old",
        "oughtn't":"ought not",
        "shalln't":"shall not",
        "shan't":"shall not",
        "she'd":"she would",
        "she'll":"she will",
        "she's":"she is",
        "shouldn't":"should not",
        "shouldn't've":"should not have",
        "should've":"should have",
        "somebody's":"somebody is",
        "someone's":"someone is",
        "something's":"something is",
        "that'd":"that would",
        "that'll":"that will",
        "that're":"that are",
        "that's":"that is",
        "there'd":"there would",
        "there'll":"there will",
        "there're":"there are",
        "there's":"there is",
        "these're":"these are",
        "they'd":"they would",
        "they'll":"they will",
        "they're":"they are",
        "they've":"they have",
        "this's":"this is",
        "those're":"those are",
        "'tis":"it is",
        "'twas":"it was",
        "wanna":"want to",
        "wasn't":"was not",
        "we'd":"we would",
        "we'd've":"we would have",
        "we'll":"we will",
        "we're":"we are",
        "weren't":"were not",
        "we've":"we have",
        "what'd":"what did",
        "what'll":"what will",
        "what're":"what are",
        "what's":"what is",
        "what've":"what have",
        "when's":"when is",
        "where'd":"where did",
        "where're":"where are",
        "where's":"where is",
        "where've":"where have",
        "which's":"which is",
        "who'd":"who would",
        "who'd've":"who would have",
        "who'll":"who will",
        "who're":"who are",
        "who's":"who is",
        "who've":"who have",
        "why'd":"why did",
        "why're":"why are",
        "why's":"why is",
        "won't":"will not",
        "wouldn't":"would not",
        "would've":"would have",
        "y'all":"you all",
        "you'd":"you would",
        "you'll":"you will",
        "you're":"you are",
        "you've":"you have",
        "Whatcha":"What are you",
        "luv":"love",
        "sux":"sucks"
        }
CONTRACTIONS = load_dict_contractions()

# Contractions/Slang cleaning
# CONTRADICTIONS is a list of contradictions and slang and their conversion.
mydata.iloc[:,0] = [j.replace("â€™","'") for j in mydata.iloc[:,0]]
words = [j.split() for j in mydata.iloc[:,0]]
for i in np.arange(len(mydata.iloc[:,0])):
  reformed = [CONTRACTIONS[word] if word in CONTRACTIONS else word for word in words[i]]
  mydata.iloc[i,0] = [" ".join(reformed)]

mydata.head()

# Commented out IPython magic to ensure Python compatibility.
# %cd ..

#np.savetxt("Fulldata_Preprocessed/half_cleaned.txt", mydata.iloc[:,0], fmt='%s')
file1 = open(r"Fulldata_Preprocessed/half_cleaned.txt", 'r')
half_mydata = file1.readlines()

half_mydata = pd.DataFrame(half_mydata) #columns=["text", "sentiment", "confidence"]

half_mydata.head()

for i in np.arange(len(half_mydata)):
  half_mydata.iloc[i,0] = half_mydata.iloc[i,0][:-2]

mydata.loc['text'] = half_mydata.iloc[:,0]

mydata.dtypes

mydata.head()

# Remove of hashtags/accounts
import re
mydata.iloc[:,0] = [' '.join(re.sub("(@[A-Za-z0-9]+)|(#[A-Za-z0-9]+)", " ", j).split()) for j in mydata.iloc[:,0]]

# Remove web addresses:
mydata.iloc[:,0] = [' '.join(re.sub(r'^https?:\/\/.*[\r\n]*', " ",j).split()) for j in mydata.iloc[:,0]]

# Convert every thing to lower case to avoid case sensitive case issue
mydata.iloc[:,0] = [j.lower() for j in mydata.iloc[:,0]]

# remove punctuation
table = str.maketrans(' ',' ',string.punctuation) # only do this for fastText
mydata.iloc[:,0] = [j.translate(table) for j in mydata.iloc[:,0]]

# remove them from the text
mydata.iloc[:,0] = [j.replace('\x96',' ') for j in mydata.iloc[:,0]]

"""Tokenize"""

# estimating the embedding
# fastText
tokenizer = Tokenizer() # create tokenizer model
tokenizer.fit_on_texts(mydata.iloc[:,0]) # Trains it over the tokens that we have

# Get words
vals = list(tokenizer.word_index.keys())

# write CSV with the output
file = codecs.open('/gdrive/My Drive/full_data_words.csv', "w", "utf-8")

for item in vals:
    file.write("%s\r\n" % item)
    
file.close()

"""compile fastText embedding"""

# Commented out IPython magic to ensure Python compatibility.
# %cd /gdrive/My Drive/fastText-0.9.1
!make

!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz

!zcat cc.en.300.bin.gz > cc.en.300.bin

# Commented out IPython magic to ensure Python compatibility.
# %cd ..
!unzip glove.6B.300d.zip glove.6B.300d.txt

# Commented out IPython magic to ensure Python compatibility.
# %cd ./My Drive/fastText-0.9.1

!ls
!chmod u+x fasttext

!./fasttext print-sentence-vectors cc.en.300.bin < ../full_data_words.csv > EmbeddingFulldata.tsv

!head EmbeddingFulldata.tsv

"""FastText ouputs space-separated words. We separate them with comma"""

import fileinput

with fileinput.FileInput('EmbeddingFulldata.tsv', inplace=True, backup='.bak') as file:
    for line in file:
        print(line.replace(' ', ','), end='')

import numpy as np
import os

# Create the first line (not sure about the number of columns)
firstLine = ','.join(['D'+str(i) for i in np.arange(1, 302)]) + '\n'

# Open as read only. Read the file
with open('EmbeddingFulldata.tsv', 'r') as original: 
  data = original.read()

# Open to write and write the first line and the rest
with open('EmbeddingFulldata.csv', 'w') as modified: 
  modified.write(firstLine + data)

data = pd.read_csv('EmbeddingFulldata.csv')
data.head()

"""Using the embedding layer

1. Read the embedding
"""

# Read word embeddings
Embeddings = pd.read_csv('EmbeddingFulldata.csv', sep=',', decimal = '.',
                         low_memory = True, index_col = False)
Embeddings.drop(columns='D301',inplace=True)
Embeddings.describe()

"""2. Calculate the One-Hot inputs (by using an "index") which will index which words are in which text."""

# Create embedding dictionary

EmbeddingsDict = dict(zip(vals, Embeddings.values))

# Commented out IPython magic to ensure Python compatibility.
import seaborn as sns
import numpy as np
# %matplotlib inline

# Count maximum number of words per file.
wordDist = [len(w.split()) for w in mydata.iloc[:,0]]
print('Avg. no of words: ' + str(np.round(np.mean(wordDist), 2)))
print('Std. deviation: ' + str(np.round(np.std(wordDist), 2)))
print('Max words: ' + str(np.max(wordDist)))


# Generate the plot
distfull = sns.distplot(wordDist)

# I'm saving the image to a PDF, as it makes it easier later to download.
distfull.figure.savefig("wordDist.pdf", format = "pdf")

"""By observing the distribution plot, we will use 60 words as maximum.

Now we create input layer, adding padding to text that are smaller than 60 or striming the longer part.
"""

#mydata[mydata['confidence'] > 1].confidence = 
mydata.loc[(mydata.confidence) > 1, 'confidence'] = 1
mydata.loc[(mydata.confidence) < 0, 'confidence'] = 0

from tensorflow.keras.preprocessing.sequence import pad_sequences

#create word index from input
sequences = tokenizer.texts_to_sequences(mydata.iloc[:,0]) #create the sequences, transforming each text in texts in a sequence of integers.

# Create the indexes. word index is a dictionary with words in it.
word_index = tokenizer.word_index
print('Found %s unique tokens.' % len(word_index))

# Create the training dataset, adding padding when necessary
data = pad_sequences(sequences, maxlen=60,
            padding='pre') # add padding at the front

# multiplied by the confidence for each text
data = pd.DataFrame(data)
#data = data.mul(mydata.iloc[:,2], axis=0)

#concatenate data with their confidence score
data_conf = pd.concat([data,mydata.iloc[:,2]], axis=1)

# Create the objective function
labels = mydata.iloc[:,1]
print('Shape of data tensor', data.shape)
print('Shape of label tensor', labels.shape)

list(word_index.keys())[-1]

# save the training dataset
#create saving directory
#!mkdir Fulldata_Preprocessed

# Save outputs
np.savetxt("Fulldata_Preprocessed/Fulldata_Padded.txt", data)
np.savetxt("Fulldata_Preprocessed/Fulldata_label.txt", labels)

data_conf.iloc[0,:]

"""Now we need to construct the Embedding matrix. This matrix will have the weights associated with each index. Keras will automatically construct the correct embedding of length 60"""

# Create the first matrix full with 0's
embedding_matrix = np.zeros((len(word_index) + 1, 300))

# Generate embeddings matrix
for word, i in word_index.items():
  embedding_vector = EmbeddingsDict.get(word)
  if embedding_vector is not None:
    #words not found in embedding index will be all-zeros.
    embedding_matrix[i] = embedding_vector

#print what came out
embedding_matrix

# save the embedding matrix
np.savetxt("Fulldata_Preprocessed/Fulldata_EmbeddingMatrix.txt", embedding_matrix)

!ls

file_embedding = open("Fulldata_Preprocessed/Fulldata_EmbeddingMatrix.txt")
embedding_matrix_1 = file_embedding.readlines()

# also save the word dictionary
# A pickle file is a Python native file
import pickle
f = open("Fulldata_Preprocessed/WordDictionary.pkl","wb") # write binary
pickle.dump(word_index, f)
f.close()

# Zip all files for download.
!zip -r Fulldata_Preprocessed.zip Fulldata_Preprocessed

"""**Modelling using an embedding layer**

Start to build a model and add embedding layer
"""

# Commented out IPython magic to ensure Python compatibility.
# %cd ./Fulldata_Preprocessed

!ls
file_pad = open(r"Fulldata_Padded.txt","r")
data = file_pad.readlines()

from sklearn.model_selection import train_test_split
# split training data and test data
X_train,X_test,y_train,y_test = train_test_split(data_conf,labels,test_size=0.33,random_state=17)

from tensorflow.keras.utils import get_file
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Input, Embedding, Reshape, MaxPooling1D
from tensorflow.keras.layers import Activation
from tensorflow.keras.layers import Flatten, Dense, Dropout, Lambda
from tensorflow.keras.layers import BatchNormalization
from tensorflow.keras.optimizers import SGD, RMSprop, Adam
from tensorflow.keras.metrics import categorical_crossentropy, categorical_accuracy
from tensorflow.keras.layers import *

model = Sequential()
embedding_layer = Embedding(len(word_index) + 1,
               300,
               weights=[embedding_matrix],
               input_length=60,   # might need change later
               trainable=False)
model.add(embedding_layer)

# Check for 64 sequences of length 5.
model.add(Conv1D(64, 1, activation = 'relu'))

# Turn output matrices into 1D tensor for shallow network.
model.add(Flatten())

# Add 64 neurons with ReLU activation.
model.add(Dense(64))

# Add dropout.
model.add(Dropout(0.45))
model.add(Activation('relu'))

# Add an output layer with a sigmoid.
model.add(Dense(1))
model.add(Activation('sigmoid'))

# Use Adam as optimizer, with a binary_crossentropy error.
model.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['acc'])

model.summary()

X_train.shape

"""Now, we will train the model"""

# Fit the model
history = model.fit(X_train.iloc[:,:60], y_train, validation_split=0.33, epochs=1, batch_size=10,sample_weight=np.array(X_train.iloc[:,60]))
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(1, len(loss)+1)

import matplotlib.pyplot as plt
plt.plot(epochs, loss, 'bo', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

"""Predict using model1, and calculate accracy, AUC, confusion matrix over the test set"""

from sklearn.metrics import confusion_matrix, roc_curve, auc

pred_y = model.predict_classes(X_test.iloc[:,:60])
pred_prob_y = model.predict(X_test.iloc[:,:60])

# confusion matrix
confusion_matrix(y_true=y_test, y_pred=pred_y)

# calculate the ROC points
fpr, tpr, threshold = roc_curve(y_test, pred_prob_y)
roc_auc = auc(fpr,tpr)
plt.plot(fpr,tpr,label='ROC curve (AUC = %0.2f)' % roc_auc)
plt.legend(loc=4)
plt.show()

# confusion matrix
confusion_matrix(y_true=y_test, y_pred=pred_y)

auc(fpr,tpr)

X_train.shape

# Filter sizes to use.
filter_sizes = (3,5)

# Initialize. We need to give it the input dimension (from the Embedding!)
graph_in = Input(shape=(60, 300))
convs = []
avgs = []

# This for stacks the layers. Inside each for, we build the sequence of layer. The command "append" adds
# that to the "conv" variable, which is simply a stack of convolutions.
for fsz in filter_sizes:
    conv = Conv1D(filters=128,
                  kernel_size=fsz,
                         padding='valid',
                         activation='relu',
                         strides=1)(graph_in) # Note the (graph_in). This means "put this layer AFTER the graph_in layer.
    pool = MaxPooling1D(pool_size=60 - fsz + 1)(conv) # Put this layer AFTER the convolution just created.
    flattenMax = Flatten()(pool) # Flatten the pooling layer.
    convs.append(flattenMax) # Append this to the convs object that saves the stack.
    
# Concatenate layers.
if len(filter_sizes)>1:
    out = Concatenate()(convs)
else:
    out = convs[0]

graph = Model(inputs=graph_in, outputs=out, name="graphModel")

graph.summary()

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
from tensorflow.keras.utils import plot_model
from IPython.display import Image
# %matplotlib inline

plot_model(graph, show_shapes=True, show_layer_names=True, to_file='GraphModel.png')
Image(retina=True, filename='GraphModel.png')

# Final model

model2 = Sequential()
model2.add(embedding_layer)
model2.add(graph)

#Dropout
model2.add(Dropout(0.5)) # 0.55

# Add 64 neurons with ReLU activation.
model2.add(Dense(64))

# Add dropout.
model2.add(Dropout(0.45))
model2.add(Activation('relu'))

# Add an output layer with a sigmoid.
model2.add(Dense(1))
model2.add(Activation('sigmoid'))

# Use Adam as optimizer, with a binary_crossentropy error.
model2.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['acc'])

model2.summary()

history2 = model2.fit(X_train.iloc[:,:60], y_train, validation_split=0.33, epochs=3, batch_size=20, sample_weight=np.array(X_train.iloc[:,60]))
loss2 = history2.history['loss']
val_loss2 = history2.history['val_loss']
epochs = range(1, len(loss2) + 1)
plt.plot(epochs, loss2, 'bo', label='Training loss')
plt.plot(epochs, val_loss2, 'b', label='Validation loss')
plt.title('Training and validation loss for model2')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

pred_y = model2.predict_classes(X_test.iloc[:,:60])
pred_prob_y = model2.predict(X_test.iloc[:,:60])


# calculate the ROC points
fpr, tpr, threshold = roc_curve(y_test, pred_prob_y)
roc_auc = auc(fpr,tpr)
plt.plot(fpr,tpr,label='ROC curve (AUC = %0.2f)' % roc_auc)
plt.legend(loc=4)
plt.show()

# confusion matrix
confusion_matrix(y_true=y_test, y_pred=pred_y)

auc(fpr, tpr)

"""Embedding GloVe"""

# Commented out IPython magic to ensure Python compatibility.
# %cd ..

embeddings_index = {}
f = open(os.path.join(".", 'glove.6B.300d.txt'))
for line in f:
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings_index[word] = coefs
f.close()

print('Found %s word vectors.' % len(embeddings_index))

embedding_matrix2 = np.zeros((len(word_index) + 1, 300))
for word, i in word_index.items():
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        # words not found in embedding index will be all-zeros.
        embedding_matrix2[i] = embedding_vector

embedding_layer2 = Embedding(len(word_index) + 1,
                300,
                weights=[embedding_matrix2],
                input_length=60,
                trainable=False)

"""Try the first model architecture"""

model_G = Sequential()
model_G.add(embedding_layer2)
model_G.add(Conv1D(64, 1, activation = 'relu'))

# Turn output matrices into 1D tensor for shallow network.
model_G.add(Flatten())
# Add 64 neurons with ReLU activation.
model_G.add(Dense(64))

# Add dropout.
model_G.add(Dropout(0.5))
model_G.add(Activation('relu'))

# Add an output layer with a sigmoid.
model_G.add(Dense(1))
model_G.add(Activation('sigmoid'))

# Use Adam as optimizer, with a binary_crossentropy error.
model_G.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['acc'])

model_G.summary()

history3 = model_G.fit(X_train.iloc[:,:60], y_train, validation_split=0.33, epochs=3, batch_size=5,sample_weight=np.array(X_train.iloc[:,60]))
loss3 = history3.history['loss']
val_loss3 = history3.history['val_loss']
epochs = range(1, len(loss3) + 1)
plt.plot(epochs, loss3, 'bo', label='Training loss')
plt.plot(epochs, val_loss3, 'b', label='Validation loss')
plt.title('Training and validation loss for model_G')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

pred_y = model_G.predict_classes(X_test.iloc[:,:60])
pred_prob_y = model_G.predict(X_test.iloc[:,:60])


# calculate the ROC points
fpr, tpr, threshold = roc_curve(y_test, pred_prob_y)
roc_auc = auc(fpr, tpr)
plt.plot(fpr,tpr,label='ROC curve (AUC = %0.2f)' % roc_auc)
plt.legend(loc=4)
plt.show()

# confusion matrix
confusion_matrix(y_true=y_test, y_pred=pred_y)

"""Try the second model"""

model_G2 = Sequential()

model_G2.add(embedding_layer2)
model_G2.add(graph)

# Add 64 neurons with ReLU activation.
model_G2.add(Dense(64))

# Add dropout.
model_G2.add(Dropout(0.5))
model_G2.add(Activation('relu'))

# Add an output layer with a sigmoid.
model_G2.add(Dense(1))
model_G2.add(Activation('sigmoid'))

# Use Adam as optimizer, with a binary_crossentropy error.
model_G2.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['acc'])

history4 = model_G2.fit(X_train.iloc[:,:60], y_train, validation_split=0.33, epochs=3, batch_size=20,sample_weight=np.array(X_train.iloc[:,60]))
loss4 = history4.history['loss']
val_loss4 = history4.history['val_loss']
epochs = range(1, len(loss4) + 1)
plt.plot(epochs, loss4, 'bo', label='Training loss')
plt.plot(epochs, val_loss4, 'b', label='Validation loss')
plt.title('Training and validation loss for model_G')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

pred_y = model_G2.predict_classes(X_test.iloc[:,:60])
pred_prob_y = model_G2.predict(X_test.iloc[:,:60])


# calculate the ROC points
fpr, tpr, threshold = roc_curve(y_test, pred_prob_y)
plt.plot(fpr,tpr,label='ROC curve (AUC = %0.2f)' % roc_auc)
plt.legend(loc=4)
plt.show()

# confusion matrix
confusion_matrix(y_true=y_test, y_pred=pred_y)